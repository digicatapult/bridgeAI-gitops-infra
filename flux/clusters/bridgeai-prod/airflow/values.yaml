airflow:
  legacyCommands: false
  image:
    repository: apache/airflow
    tag: 2.8.4-python3.9
  executor: KubernetesExecutor
  config:
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "False"
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    AIRFLOW__WEBSERVER__BASE_URL: "https://airflow.dc-mlops.co.uk"
  users:
    - username: admin
      role: Admin
      email: declined_admin@digicatapult.org.uk
      firstName: admin
      lastName: admin
    - username: airflow-prod-user
      email: declined_user@digicatapult.org.uk
      role: User
      firstName: user
      lastName: user
  usersUpdate: true
  connections:
    - id: local-k8s
      type: kubernetes
      extra: |
        {
          "in_cluster": true,
          "namespace": "airflow",
          "disable_verify_ssl": true
        }
  connectionsUpdate: true
  variables:
    - key: "data_path"
      value: "/data"
    - key: "data_url"
      value: "https://raw.githubusercontent.com/renjith-digicat/random_file_shares/main/HousingData.csv"
    - key: "namespace"
      value: "airflow"
    - key: "base_image_model_training"
      value: "digicatapult/bridgeai-regression-model-training:latest"
    - key: "base_image_data_ingestion"
      value: "digicatapult/bridgeai-regression-model-data-ingestion:latest"
    - key: "base_image_model_image_generation"
      value: "digicatapult/bridgeai-model-baseimage:latest"
    - key: "base_image_drift_monitoring"
      value: "digicatapult/bridgeai-drift-monitoring:latest"
    - key: "is_base_image_authenticated"
      value: "False"
    - key: "model_docker_build_context_pvc"
      value: "model-docker-build-context-pvc"
    - key: "deploy_model_name"
      value: "house_price_prediction_prod"
    - key: "deploy_model_alias"
      value: "champion"
    - key: "mlflow_docker_out_dir"
      value: "./mlflow-dockerfile"
    - key: "mlflow_built_image_name"
      value: "bridgeai-mlops"
    - key: "mlflow_built_image_tag"
      value: "latest"
    - key: "docker_registry_for_model_image"
      value: "058264114863.dkr.ecr.eu-west-2.amazonaws.com"
    - key: "mlflow_tracking_uri"
      value: "https://mlflow.dc-mlops.co.uk"
    - key: "connection_id"
      value: "local-k8s"
    - key: "in_cluster"
      value: true
    - key: "model_training_configmap"
      value: "model-training-configmap"
    - key: "data_ingestion_configmap"
      value: "data-ingest-configmap"
    - key: "github_secret"
      value: "github-auth"
    - key: "data_ingestion_pvc"
      value: "data-ingestion-pvc"
    - key: "model_training_pvc"
      value: "model-training-pvc"
    - key: "dvc_remote"
      value: "s3://bridgeai-dvc-remote"
    - key: "dvc_endpoint_url"
      value: "https://bridgeai-dvc-remote.s3.eu-west-2.amazonaws.com"
    - key: "dvc_access_key_id"
      value: "admin"
    - key: "dvc_secret_access_key"
      value: "password"
    - key: "data_version"
      value: "data-v1.0.0"
    - key: "data_repo"
      value: "https://github.com/digicatapult/bridgeAI-regression-model-data-ingestion.git"
    - key: "historical_data_version"
      value: "data-v1.0.0"
    - key: "new_data_version"
      value: "data-v1.1.0"
    - key: "drift_monitoring_configmap"
      value: "drift-monitoring-configmap"
    - key: "drift_monitoring_pvc"
      value: "drift-monitoring-pvc"
    - key: "drift_report_bucket"
      value: "bridgeai-evidently-reports"
    - key: "model_endpoint"
      value: "http://host.docker.internal:5001/invocations"
    - key: "docker_build_pod_resource_limits_enabled"
      value: "True"
    # Resource variables ensure that nodes have sufficient resources
    - key: "docker_build_pod_limit_cpu"
      value: "1"
    - key: "docker_build_pod_limit_memory"
      value: "12Gi"
    - key: "docker_build_pod_request_cpu"
      value: "2"
    - key: "docker_build_pod_request_eph_storage"
      value: "20Gi"
    - key: "docker_build_pod_request_memory"
      value: "4Gi"
    # Node label selection
    - key: "docker_build_pod_node_label_enabled"
      value: "True"
    - key: "docker_build_pod_node_label"
      value: '["t3.2xlarge"]'
    # Secret variables need populating here. Assignment is done using extraEnv.
    - key: "mlflow_tracking_username"
      value: "mlflow-prod-user-credentials.username"
    - key: "mlflow_tracking_password"
      value: "mlflow-prod-user-credentials.password"
    - key: "docker_reg_secret"
      value: "ghcr-io"
    - key: "model_docker_push_secret"
      value: "ecr-credentials"
    - key: "github_secret_username_key"
      value: "username"
    - key: "github_secret_password_key"
      value: "password"
  variablesUpdate: false
  extraEnv:
    - name: mlflow_tracking_username
      valueFrom:
        secretKeyRef:
          name: mlflow-prod-user-credentials
          key: username
    - name: mlflow_tracking_password
      valueFrom:
        secretKeyRef:
          name: mlflow-prod-user-credentials
          key: password
    - name: docker_reg_secret
      valueFrom:
        secretKeyRef:
          name: ghcr-io
          key: .dockerconfigjson
    - name: model_docker_push_secret
      valueFrom:
        secretKeyRef:
          name: ecr-credentials
          key: .dockerconfigjson
    - name: github_secret_password_key
      valueFrom:
        secretKeyRef:
          name: github-auth
          key: username
    - name: github_secret_username_key
      valueFrom:
        secretKeyRef:
          name: github-auth
          key: password

# Logs
scheduler:
  nodeSelector:
    node.kubernetes.io/instance-type: "t3.large"
  replicas: 1
  logCleanup:
    enabled: false
    retentionMinutes: 21600
  livenessProbe:
    enabled: false
workers:
  enabled: false
  logCleanup:
    enabled: false
  safeToEvict: false
logs:
  path: /opt/airflow/logs
  persistence:
    enabled: true
    storageClass: "efs-sc"
    accessMode: ReadWriteMany

# DAGs
dags:
  nodeSelector:
    node.kubernetes.io/instance-type: "t3.large"
  path: /opt/airflow/dags
  persistence:
    enabled: false
  gitSync:
    enabled: true
    repo: https://github.com/digicatapult/bridgeAI-airflow-DAGs.git
    repoSubPath: "dags/"
    branch: main
    httpSecret: "github-auth"
    httpSecretUsernameKey: "username"
    httpSecretPasswordKey: "password"

# Triggerer
triggerer:
  nodeSelector:
    node.kubernetes.io/instance-type: "t3.large"
  enabled: true
  replicas: 1
  capacity: 1000

# Service ingress
ingress:
  enabled: true
  apiVersion: networking.k8s.io/v1
  web:
    enabled: true
    replicas: 1
    host: "airflow.dc-mlops.co.uk"
    ingressClassName: "nginx-airflow"
flower:
  enabled: false

webserver:
  resources:
    limits:
      memory: 512Mi
    requests:
      memory: 128Mi

# Identities
serviceAccount:
  create: true
  name: "airflow"
  annotations:
    eks.amazonaws.com/role-arn: "arn:aws:iam::058264114863:role/airflow-access-role"

# Database(s)
pgbouncer:
  enabled: false
  resources:
    requests:
      memory: "128Mi"
      cpu: "100m"
    limits:
      memory: "1Gi"
      cpu: "1000m"
# [WARNING] the embedded Postgres is NOT SUITABLE for production deployments of Airflow
postgresql:
  enabled: true
  persistence:
    enabled: true
    storageClass: "gp2"
# [WARNING] consider using an external database with `externalDatabase.*`
redis:
  enabled: false
